{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Funciones Reutilizables del Proyecto\n",
        "\n",
        "Este notebook contiene funciones útiles que pueden ser reutilizadas en cualquier parte del proyecto de análisis de energía renovable.\n",
        "\n",
        "## Contenido:\n",
        "1. **Carga y preprocesamiento de datos**\n",
        "2. **Visualización**\n",
        "3. **Métricas de evaluación**\n",
        "4. **Utilidades generales**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importaciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuración de visualización\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Funciones de Carga y Preprocesamiento de Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_capacidad_data(file_path, sep=';', decimal=','):\n",
        "    \"\"\"\n",
        "    Carga el archivo CSV de capacidad acumulada con el formato específico del proyecto.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Ruta al archivo CSV\n",
        "    sep : str\n",
        "        Delimitador (default: ';')\n",
        "    decimal : str\n",
        "        Separador decimal (default: ',')\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame con los datos cargados y procesados\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path, sep=sep, decimal=decimal, encoding='utf-8-sig')\n",
        "    \n",
        "    # Convertir la columna Category a datetime\n",
        "    df['Category'] = pd.to_datetime(df['Category'], format='%Y-%m-%d')\n",
        "    \n",
        "    # Ordenar por fecha\n",
        "    df = df.sort_values('Category').reset_index(drop=True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def load_capacidad_csv_data(file_path, sep=';', decimal=','):\n",
        "    \"\"\"\n",
        "    Carga el archivo CSV de capacidad acumulada con el formato específico del proyecto.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Ruta al archivo CSV\n",
        "    sep : str\n",
        "        Delimitador (default: ';')\n",
        "    decimal : str\n",
        "        Separador decimal (default: ',')\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame con los datos cargados y procesados\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path, sep=sep, decimal=decimal, encoding='utf-8-sig')\n",
        "    \n",
        "    return df\n",
        "\n",
        "def preprocess_capacidad_data(df):\n",
        "    \"\"\"\n",
        "    Preprocesa los datos de capacidad acumulada.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame con los datos de capacidad\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame preprocesado\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # Crear columnas de tiempo\n",
        "    df_processed['Year'] = df_processed['Category'].dt.year\n",
        "    df_processed['Month'] = df_processed['Category'].dt.month\n",
        "    df_processed['Day'] = df_processed['Category'].dt.day\n",
        "    df_processed['DayOfYear'] = df_processed['Category'].dt.dayofyear\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "def extract_first_day_per_month(df, date_column='Category', verbose=True):\n",
        "    \"\"\"\n",
        "    Extrae el primer día disponible de cada mes desde un DataFrame de datos temporales.\n",
        "    \n",
        "    Esta función procesa un DataFrame con datos temporales y retorna un nuevo DataFrame que contiene\n",
        "    solo el primer día disponible de cada mes para todos los años, reduciendo significativamente\n",
        "    la cantidad de datos a procesar.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame con datos temporales. Debe contener una columna de fechas.\n",
        "    date_column : str\n",
        "        Nombre de la columna que contiene las fechas (default: 'Category').\n",
        "        La columna debe ser de tipo datetime o convertible a datetime.\n",
        "    verbose : bool\n",
        "        Si es True, imprime información del proceso (default: True)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame con solo el primer día disponible de cada mes, ordenado por fecha\n",
        "    \n",
        "    Example:\n",
        "    --------\n",
        "    >>> # Cargar datos\n",
        "    >>> df = load_capacidad_data('capacidad_acumulada.csv')\n",
        "    >>> \n",
        "    >>> # Extraer primer día de cada mes\n",
        "    >>> df_reduced = extract_first_day_per_month(df, date_column='Category')\n",
        "    >>> \n",
        "    >>> # Ver resultados\n",
        "    >>> print(f\"Registros originales: {len(df)}\")\n",
        "    >>> print(f\"Registros reducidos: {len(df_reduced)}\")\n",
        "    \"\"\"\n",
        "    # Hacer una copia para no modificar el DataFrame original\n",
        "    df_work = df.copy()\n",
        "    \n",
        "    # Asegurar que la columna de fecha sea datetime\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df_work[date_column]):\n",
        "        df_work[date_column] = pd.to_datetime(df_work[date_column], errors='coerce')\n",
        "    \n",
        "    # Filtrar filas con fechas válidas\n",
        "    df_work = df_work.dropna(subset=[date_column])\n",
        "    \n",
        "    if len(df_work) == 0:\n",
        "        if verbose:\n",
        "            print(\"⚠ Advertencia: No se encontraron fechas válidas en el DataFrame\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Crear columnas auxiliares de año y mes\n",
        "    df_work['_Year'] = df_work[date_column].dt.year\n",
        "    df_work['_Month'] = df_work[date_column].dt.month\n",
        "    \n",
        "    # Agrupar por año y mes, y tomar el primer día (mínimo) de cada grupo\n",
        "    first_day_per_month = df_work.groupby(['_Year', '_Month']).apply(\n",
        "        lambda x: x.loc[x[date_column].idxmin()]\n",
        "    ).reset_index(drop=True)\n",
        "    \n",
        "    # Eliminar las columnas auxiliares\n",
        "    first_day_per_month = first_day_per_month.drop(columns=['_Year', '_Month'])\n",
        "    \n",
        "    # Ordenar por fecha\n",
        "    first_day_per_month = first_day_per_month.sort_values(date_column).reset_index(drop=True)\n",
        "    \n",
        "    # Información del proceso\n",
        "    if verbose:\n",
        "        total_records = len(df)\n",
        "        reduced_records = len(first_day_per_month)\n",
        "        reduction_pct = ((total_records - reduced_records) / total_records * 100) if total_records > 0 else 0\n",
        "        print(f\"✓ Registros originales: {total_records}\")\n",
        "        print(f\"✓ Registros reducidos: {reduced_records}\")\n",
        "        print(f\"✓ Reducción: {reduction_pct:.1f}% ({total_records} → {reduced_records} registros)\")\n",
        "    \n",
        "    return first_day_per_month\n",
        "\n",
        "def impute_missing_values(df, columns=None, method='forward_fill', date_column='Category', **kwargs):\n",
        "    \"\"\"\n",
        "    Imputa valores faltantes en las columnas especificadas usando diferentes métodos.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame con datos temporales\n",
        "    columns : list, optional\n",
        "        Lista de columnas a imputar. Si es None, imputa SOLAR, EOLICA, PCH\n",
        "    method : str\n",
        "        Método de imputación. Opciones:\n",
        "        - 'forward_fill': Forward fill (rellena hacia adelante)\n",
        "        - 'backward_fill': Backward fill (rellena hacia atrás)\n",
        "        - 'linear': Interpolación lineal\n",
        "        - 'polynomial': Interpolación polinomial (grado especificado en kwargs)\n",
        "        - 'spline': Interpolación spline (orden especificado en kwargs)\n",
        "        - 'mean': Media de la columna\n",
        "        - 'median': Mediana de la columna\n",
        "        - 'knn': K-Nearest Neighbors (n_neighbors especificado en kwargs, default=5)\n",
        "    date_column : str\n",
        "        Nombre de la columna de fecha para ordenar los datos (default: 'Category')\n",
        "    **kwargs\n",
        "        Argumentos adicionales para métodos específicos:\n",
        "        - order: para spline (default: 3)\n",
        "        - degree: para polynomial (default: 2)\n",
        "        - n_neighbors: para knn (default: 5)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame con valores imputados\n",
        "    \n",
        "    Example:\n",
        "    --------\n",
        "    >>> df_imputed = impute_missing_values(df, method='linear')\n",
        "    >>> df_imputed = impute_missing_values(df, method='knn', n_neighbors=3)\n",
        "    \"\"\"\n",
        "    from sklearn.impute import KNNImputer\n",
        "    \n",
        "    df_imputed = df.copy()\n",
        "    \n",
        "    # Columnas por defecto\n",
        "    if columns is None:\n",
        "        columns = ['SOLAR', 'EOLICA', 'PCH']\n",
        "    \n",
        "    # Filtrar solo las columnas que existen en el DataFrame\n",
        "    columns = [col for col in columns if col in df_imputed.columns]\n",
        "    \n",
        "    if len(columns) == 0:\n",
        "        print(\"⚠ Advertencia: No se encontraron columnas válidas para imputar\")\n",
        "        return df_imputed\n",
        "    \n",
        "    # Asegurar que los datos estén ordenados por fecha\n",
        "    if date_column in df_imputed.columns:\n",
        "        df_imputed = df_imputed.sort_values(date_column).reset_index(drop=True)\n",
        "    \n",
        "    # Contar valores faltantes antes de imputar\n",
        "    missing_before = df_imputed[columns].isnull().sum()\n",
        "    total_missing = missing_before.sum()\n",
        "    \n",
        "    if total_missing == 0:\n",
        "        print(\"✓ No hay valores faltantes para imputar\")\n",
        "        return df_imputed\n",
        "    \n",
        "    print(f\"Valores faltantes antes de imputar:\")\n",
        "    for col in columns:\n",
        "        if missing_before[col] > 0:\n",
        "            print(f\"  - {col}: {missing_before[col]} valores faltantes\")\n",
        "    \n",
        "    # Aplicar método de imputación\n",
        "    for col in columns:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            if method == 'forward_fill':\n",
        "                df_imputed[col] = df_imputed[col].ffill()\n",
        "            \n",
        "            elif method == 'backward_fill':\n",
        "                df_imputed[col] = df_imputed[col].bfill()\n",
        "            \n",
        "            elif method == 'linear':\n",
        "                df_imputed[col] = df_imputed[col].interpolate(method='linear', limit_direction='both')\n",
        "            \n",
        "            elif method == 'polynomial':\n",
        "                degree = kwargs.get('degree', 2)\n",
        "                df_imputed[col] = df_imputed[col].interpolate(method='polynomial', order=degree, limit_direction='both')\n",
        "            \n",
        "            elif method == 'spline':\n",
        "                order = kwargs.get('order', 3)\n",
        "                df_imputed[col] = df_imputed[col].interpolate(method='spline', order=order, limit_direction='both')\n",
        "            \n",
        "            elif method == 'mean':\n",
        "                mean_value = df_imputed[col].mean()\n",
        "                df_imputed[col] = df_imputed[col].fillna(mean_value)\n",
        "            \n",
        "            elif method == 'median':\n",
        "                median_value = df_imputed[col].median()\n",
        "                df_imputed[col] = df_imputed[col].fillna(median_value)\n",
        "            \n",
        "            elif method == 'knn':\n",
        "                n_neighbors = kwargs.get('n_neighbors', 5)\n",
        "                # Preparar datos para KNN (solo columnas numéricas)\n",
        "                numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "                if col in numeric_cols:\n",
        "                    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
        "                    # Imputar solo las columnas numéricas\n",
        "                    df_temp = df_imputed[numeric_cols].copy()\n",
        "                    df_imputed_values = imputer.fit_transform(df_temp)\n",
        "                    df_imputed[numeric_cols] = pd.DataFrame(df_imputed_values, columns=numeric_cols, index=df_temp.index)\n",
        "            \n",
        "            else:\n",
        "                print(f\"⚠ Método '{method}' no reconocido. Usando forward_fill por defecto.\")\n",
        "                df_imputed[col] = df_imputed[col].ffill()\n",
        "    \n",
        "    # Contar valores faltantes después de imputar\n",
        "    missing_after = df_imputed[columns].isnull().sum()\n",
        "    total_missing_after = missing_after.sum()\n",
        "    \n",
        "    print(f\"\\n✓ Imputación completada usando método: {method}\")\n",
        "    print(f\"✓ Valores faltantes después: {total_missing_after}\")\n",
        "    if total_missing_after > 0:\n",
        "        print(\"⚠ Advertencia: Aún quedan valores faltantes después de la imputación\")\n",
        "        for col in columns:\n",
        "            if missing_after[col] > 0:\n",
        "                print(f\"  - {col}: {missing_after[col]} valores faltantes\")\n",
        "    \n",
        "    return df_imputed\n",
        "\n",
        "def compare_imputation_methods(df, columns=None, date_column='Category', methods=None):\n",
        "    \"\"\"\n",
        "    Compara diferentes métodos de imputación y muestra estadísticas.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame con datos temporales\n",
        "    columns : list, optional\n",
        "        Lista de columnas a comparar. Si es None, usa SOLAR, EOLICA, PCH\n",
        "    date_column : str\n",
        "        Nombre de la columna de fecha\n",
        "    methods : list, optional\n",
        "        Lista de métodos a comparar. Si es None, usa métodos por defecto\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Diccionario con DataFrames imputados para cada método\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = ['SOLAR', 'EOLICA', 'PCH']\n",
        "    \n",
        "    if methods is None:\n",
        "        methods = ['forward_fill', 'backward_fill', 'linear', 'mean', 'median']\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"COMPARACIÓN DE MÉTODOS DE IMPUTACIÓN\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for method in methods:\n",
        "        print(f\"\\n--- Método: {method} ---\")\n",
        "        df_imputed = impute_missing_values(df.copy(), columns=columns, \n",
        "                                          method=method, date_column=date_column)\n",
        "        results[method] = df_imputed\n",
        "        \n",
        "        # Estadísticas después de imputar\n",
        "        for col in columns:\n",
        "            if col in df_imputed.columns:\n",
        "                mean_val = df_imputed[col].mean()\n",
        "                std_val = df_imputed[col].std()\n",
        "                print(f\"  {col}: Media={mean_val:.2f}, Std={std_val:.2f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Funciones de Visualización\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_time_series(df, columns=None, figsize=(15, 8), title='Serie Temporal'):\n",
        "    \"\"\"\n",
        "    Grafica series temporales de las columnas especificadas.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame con datos temporales\n",
        "    columns : list\n",
        "        Lista de columnas a graficar (default: None, grafica todas las numéricas)\n",
        "    figsize : tuple\n",
        "        Tamaño de la figura\n",
        "    title : str\n",
        "        Título del gráfico\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        columns = [col for col in columns if col not in ['Year', 'Month', 'Day', 'DayOfYear']]\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    \n",
        "    for col in columns:\n",
        "        if col in df.columns and col != 'Category':\n",
        "            ax.plot(df['Category'], df[col], label=col, linewidth=2)\n",
        "    \n",
        "    ax.set_xlabel('Fecha', fontsize=12)\n",
        "    ax.set_ylabel('Valor', fontsize=12)\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "    ax.legend(loc='best')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_correlation_matrix(df, columns=None, figsize=(10, 8)):\n",
        "    \"\"\"\n",
        "    Grafica la matriz de correlación.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame con los datos\n",
        "    columns : list\n",
        "        Lista de columnas a incluir (default: None, todas las numéricas)\n",
        "    figsize : tuple\n",
        "        Tamaño de la figura\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    \n",
        "    corr_matrix = df[columns].corr()\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
        "    ax.set_title('Matriz de Correlación', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return corr_matrix\n",
        "\n",
        "\n",
        "def graph_moving_averages(medias_moviles, datos_originales=None, titulo='Promedios Móviles - Comparación de Ventanas'):\n",
        "    \"\"\"\n",
        "    Genera una matriz de subplots con los promedios móviles.\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    medias_moviles : list\n",
        "        Lista de tuplas (serie, titulo) o solo list de series\n",
        "        Ejemplo: [(ma_2, 'MA(2)'), (ma_3, 'MA(3)')]\n",
        "    datos_originales : pd.Series o pd.DataFrame, opcional\n",
        "        Datos originales para comparar\n",
        "    \"\"\"\n",
        "    n = len(medias_moviles)\n",
        "    \n",
        "    # Convertir a formato estándar si es necesario\n",
        "    if n > 0 and not isinstance(medias_moviles[0], tuple):\n",
        "        medias_moviles = [(ma, f'Media Móvil {i+1}') for i, ma in enumerate(medias_moviles)]\n",
        "    \n",
        "    # Calcular dimensiones\n",
        "    cols = int(np.ceil(np.sqrt(n)))\n",
        "    rows = int(np.ceil(n / cols))\n",
        "    \n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 5*rows))\n",
        "    fig.suptitle(titulo, fontsize=16)\n",
        "    \n",
        "    # Aplanar axes si es necesario\n",
        "    if rows == 1:\n",
        "        axes = [axes] if cols == 1 else axes\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "    \n",
        "    for idx, (ma, titulo) in enumerate(medias_moviles):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        if datos_originales is not None:\n",
        "            ax.plot(datos_originales, label='Datos originales', alpha=0.7)\n",
        "        ax.plot(ma, label=titulo, linewidth=2)\n",
        "        ax.set_title(titulo)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Ocultar subplots vacíos\n",
        "    for idx in range(n, len(axes)):\n",
        "        axes[idx].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Funciones de Métricas de Evaluación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calcula métricas de evaluación para modelos de regresión.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_true : array-like\n",
        "        Valores reales\n",
        "    y_pred : array-like\n",
        "        Valores predichos\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Diccionario con las métricas calculadas\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    \n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    # MAPE (Mean Absolute Percentage Error)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\n",
        "    \n",
        "    metrics = {\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'MAPE': mape\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "def print_metrics(metrics):\n",
        "    \"\"\"\n",
        "    Imprime las métricas de evaluación de forma formateada.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    metrics : dict\n",
        "        Diccionario con las métricas\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"MÉTRICAS DE EVALUACIÓN\")\n",
        "    print(\"=\" * 50)\n",
        "    for key, value in metrics.items():\n",
        "        if key == 'R2':\n",
        "            print(f\"{key:15s}: {value:10.4f}\")\n",
        "        elif key == 'MAPE':\n",
        "            print(f\"{key:15s}: {value:10.2f}%\")\n",
        "        else:\n",
        "            print(f\"{key:15s}: {value:10.4f}\")\n",
        "    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Funciones de Utilidades Generales\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data_info(df):\n",
        "    \"\"\"\n",
        "    Muestra información resumida del DataFrame.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame a analizar\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"INFORMACIÓN DEL DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Forma del dataset: {df.shape}\")\n",
        "    print(f\"Filas: {df.shape[0]}\")\n",
        "    print(f\"Columnas: {df.shape[1]}\")\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(\"TIPOS DE DATOS:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(df.dtypes)\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(\"VALORES FALTANTES:\")\n",
        "    print(\"-\" * 60)\n",
        "    missing = df.isnull().sum()\n",
        "    if missing.sum() > 0:\n",
        "        print(missing[missing > 0])\n",
        "    else:\n",
        "        print(\"No hay valores faltantes\")\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(\"ESTADÍSTICAS DESCRIPTIVAS:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(df.describe())\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "def save_results(df, file_path, sep=';', decimal=','):\n",
        "    \"\"\"\n",
        "    Guarda un DataFrame en formato CSV con el formato del proyecto.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame a guardar\n",
        "    file_path : str\n",
        "        Ruta donde guardar el archivo\n",
        "    sep : str\n",
        "        Delimitador (default: ';')\n",
        "    decimal : str\n",
        "        Separador decimal (default: ',')\n",
        "    \"\"\"\n",
        "    df.to_csv(file_path, sep=sep, decimal=decimal, index=False, encoding='utf-8')\n",
        "    print(f\"Resultados guardados en: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Ejemplo de Uso\n",
        "\n",
        "Para usar estas funciones en otros notebooks, puedes importarlas así:\n",
        "    \n",
        "```python\n",
        "# Opción 1: Ejecutar este notebook completo y luego importar\n",
        "%run ../utils/funcions.ipynb\n",
        "\n",
        "# Opción 2: Convertir a módulo Python y luego importar\n",
        "# from utils.funcions import load_capacidad_data, plot_time_series, calculate_metrics\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Ejemplo Práctico\n",
        "\n",
        "A continuación se muestra un ejemplo de cómo usar las funciones:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo de uso (descomentar para ejecutar)\n",
        "# \n",
        "# # 1. Cargar datos\n",
        "# df = load_capacidad_data('../data/capacidad_acumulada.csv')\n",
        "# print(f\"Registros originales: {len(df)}\")\n",
        "# \n",
        "# # 2. Extraer primer día de cada mes (reducir datos)\n",
        "# df_reduced = extract_first_day_per_month(df, date_column='Category')\n",
        "# print(f\"Registros reducidos: {len(df_reduced)}\")\n",
        "# \n",
        "# # 3. Preprocesar\n",
        "# df_processed = preprocess_capacidad_data(df_reduced)\n",
        "# \n",
        "# # 4. Ver información\n",
        "# get_data_info(df_processed)\n",
        "# \n",
        "# # 5. Visualizar series temporales\n",
        "# plot_time_series(df_processed, columns=['SOLAR', 'EOLICA', 'PCH'], \n",
        "#                  title='Capacidad Acumulada de Energía Renovable')\n",
        "# \n",
        "# # 6. Matriz de correlación\n",
        "# corr_matrix = plot_correlation_matrix(df_processed, columns=['SOLAR', 'EOLICA', 'PCH'])\n",
        "# \n",
        "# # 7. Guardar resultados si es necesario\n",
        "# # save_results(df_reduced, '../data/capacidad_acumulada2.csv')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
